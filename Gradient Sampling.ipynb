{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "33592031-8321-4d45-beae-9536952cf7f2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.anomaly_mode.set_detect_anomaly at 0x7f3918187eb0>"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from PIL import Image\n",
    "from pathlib import Path\n",
    "from datasets import load_dataset\n",
    "from decoding.listener import CLIPListener\n",
    "import open_clip\n",
    "import torch\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0742ec56-9212-4736-ae85-ebf8206e94d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_path = \"/data/tir/projects/tir3/users/svadugur/pragmatic-clip/image-sets\"\n",
    "images_path = Path(images_path) if isinstance(images_path, str) else images_path\n",
    "\n",
    "dataset = (\n",
    "    load_dataset(\"BennoKrojer/ImageCoDe\", split=\"validation\")\n",
    "    .filter(lambda x: \"open-images\" in x[\"image_set\"])\n",
    "    .map(lambda x: {\"captions\": [x[\"description\"] for i in range(2)]})\n",
    "    .to_dict()\n",
    ")\n",
    "\n",
    "test_predictions = [\n",
    "    {k: dataset[k][i] for k in dataset} for i in range(len(dataset[\"image_set\"]))\n",
    "]\n",
    "\n",
    "model, _, preprocess = open_clip.create_model_and_transforms(\n",
    "    \"ViT-B-32\", pretrained=\"openai\", device=\"cuda:0\", precision=\"bf16\"\n",
    ")\n",
    "tokenizer = open_clip.get_tokenizer(\"ViT-B-32\")\n",
    "listener = CLIPListener(model, preprocess, tokenizer, device=\"cuda:0\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "2f7b527d-bac1-4d5c-9e4e-1fcc1ef480f7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import torch.optim as optim\n",
    "from torch import nn\n",
    "from open_clip.transformer import text_global_pool\n",
    "\n",
    "x = test_predictions[0]\n",
    "images = [\n",
    "    Image.open(images_path / x[\"image_set\"] / f\"img{i}.jpg\") for i in range(10)\n",
    "]\n",
    "texts = x['captions']\n",
    "\n",
    "with torch.no_grad():\n",
    "    text_inputs = tokenizer(texts).to(listener.device)\n",
    "    cast_dtype = listener.model.transformer.get_cast_dtype()\n",
    "    pooled_text_features, seq_text_features = listener.encode_texts(texts)\n",
    "    image_embeddings = listener.encode_images([images])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "f0f48c04-d75f-4256-a790-36de5dbb5029",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 0: Loss = 6.90625\n",
      "Step 1: Loss = 4.0625\n",
      "Step 2: Loss = 3.453125\n",
      "Step 3: Loss = 3.234375\n",
      "Step 4: Loss = 3.109375\n",
      "Step 5: Loss = 3.015625\n",
      "Step 6: Loss = 2.9375\n",
      "Step 7: Loss = 2.875\n",
      "Step 8: Loss = 2.8125\n",
      "Step 9: Loss = 2.75\n"
     ]
    }
   ],
   "source": [
    "new_text_features = seq_text_features.clone()\n",
    "lr = 10\n",
    "\n",
    "for i in range(10):\n",
    "    text_features = new_text_features.detach()\n",
    "    text_features.requires_grad_(True).retain_grad()\n",
    "\n",
    "    # Get pooled features\n",
    "    pooled_text_features, _ = text_global_pool(text_features, text_inputs, listener.model.text_pool_type)\n",
    "    if listener.model.text_projection is not None:\n",
    "        if isinstance(listener.model.text_projection, nn.Linear):\n",
    "            pooled_text_features = listener.model.text_projection(pooled_text_features)\n",
    "        else:\n",
    "            pooled_text_features = pooled_text_features @ listener.model.text_projection\n",
    "    pooled_text_features = pooled_text_features / pooled_text_features.norm(dim=-1, keepdim=True)\n",
    "\n",
    "    text_logits = listener.model.logit_scale * image_embeddings @ pooled_text_features.T\n",
    "    targets = torch.arange(text_logits.size(0), device=text_logits.device)\n",
    "    targets_one_hot = torch.nn.functional.one_hot(targets, num_classes=text_logits.size(1))\n",
    "    loss = torch.nn.functional.cross_entropy(text_logits[0, ...], targets_one_hot[0, :], reduction='sum')\n",
    "    loss.backward()\n",
    "    # TODO(jykoh): text_grads will be all zeros except for the eot token index\n",
    "    # Is this intended?\n",
    "    text_grads = text_features.grad  # (len(texts), 77, 512)\n",
    "    \n",
    "    # Take a step in the negative gradient direction\n",
    "    new_text_features = text_features - lr * text_grads\n",
    "    \n",
    "    print(f\"Step {i}: Loss = {loss.item()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9ac1cd8-2900-4f3f-8c79-1381d2150f40",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
